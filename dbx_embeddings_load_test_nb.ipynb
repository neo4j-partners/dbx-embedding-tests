{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-01",
   "metadata": {},
   "source": [
    "# Embedding Pipeline - Databricks Hosted Models\n",
    "\n",
    "This notebook loads data from a Delta table into Neo4j with vector embeddings\n",
    "using **Databricks Foundation Model APIs** (hosted models like BGE and GTE).\n",
    "\n",
    "## Available Hosted Models\n",
    "\n",
    "| Model | Dimensions | Context Length | Notes |\n",
    "|-------|------------|----------------|-------|\n",
    "| `databricks-bge-large-en` | 1024 | 512 tokens | Normalized embeddings |\n",
    "| `databricks-gte-large-en` | 1024 | 8192 tokens | Longer documents |\n",
    "\n",
    "## Advantages of Hosted Models\n",
    "\n",
    "- No deployment needed (ready to use immediately)\n",
    "- Managed scaling and availability\n",
    "- Pay-per-token pricing\n",
    "- OpenAI-compatible API format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header-04",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "path-setup-00",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PATH SETUP - Ensures modules are importable from the notebook\n",
    "# =============================================================================\n",
    "# This cell adds the notebook's directory to sys.path if needed\n",
    "# Required when running in Databricks Repos or when modules aren't on the path\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the directory containing this notebook\n",
    "# In Databricks, use the notebook path to find the module directory\n",
    "try:\n",
    "    # Try to get the notebook path from Databricks context\n",
    "    notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "    notebook_dir = \"/Workspace\" + \"/\".join(notebook_path.split(\"/\")[:-1])\n",
    "    \n",
    "    if notebook_dir not in sys.path:\n",
    "        sys.path.insert(0, notebook_dir)\n",
    "        print(f\"Added to sys.path: {notebook_dir}\")\n",
    "except Exception:\n",
    "    # Fallback: add current working directory\n",
    "    cwd = os.getcwd()\n",
    "    if cwd not in sys.path:\n",
    "        sys.path.insert(0, cwd)\n",
    "        print(f\"Added to sys.path: {cwd}\")\n",
    "\n",
    "print(\"Path setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "imports-05",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Import from modular components\n",
    "from load_utils import (\n",
    "    Config,\n",
    "    load_config,\n",
    "    neo4j_driver,\n",
    "    print_config,\n",
    "    print_section_header,\n",
    "    test_neo4j_connection,\n",
    "    format_duration,\n",
    ")\n",
    "\n",
    "from neo4j_schema import (\n",
    "    SchemaConfig,\n",
    "    setup_neo4j_schema,\n",
    "    wait_for_vector_index,\n",
    ")\n",
    "\n",
    "from embedding_providers import (\n",
    "    EmbeddingConfig,\n",
    "    DatabricksHostedEmbeddingProvider,\n",
    "    generate_query_embedding,\n",
    ")\n",
    "\n",
    "from streaming_pipeline import (\n",
    "    PipelineConfig,\n",
    "    run_pipeline,\n",
    "    print_pipeline_summary,\n",
    ")\n",
    "\n",
    "print(\"Imports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header-02",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Adjust these settings to control the embedding pipeline."
   ]
  },
  {
   "cell_type": "code",
   "id": "config-03",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE CONFIGURATION - Modify these values to control the run\n",
    "# =============================================================================\n",
    "\n",
    "# Row limit: Set to a positive number to limit rows, or -1 for all rows\n",
    "# Default: 500 rows for quick testing\n",
    "MAX_ROWS = 500  # Set to -1 to process all rows\n",
    "\n",
    "# Batch size: Number of rows per Neo4j transaction\n",
    "# Larger = faster but more memory; Smaller = safer but slower\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "# Write partitions: Parallel writers to Neo4j\n",
    "# 1 = serial (safest), 2-4 = moderate parallelism\n",
    "WRITE_PARTITIONS = 1\n",
    "\n",
    "# Databricks hosted embedding model\n",
    "# Options: \"databricks-bge-large-en\" or \"databricks-gte-large-en\"\n",
    "EMBEDDING_ENDPOINT = \"databricks-bge-large-en\"\n",
    "\n",
    "# Embedding dimensions (BGE and GTE both use 1024)\n",
    "EMBEDDING_DIMENSIONS = 1024\n",
    "\n",
    "# Neo4j node label (separate from custom model to avoid conflicts)\n",
    "NODE_LABEL = \"RemovalEventDBX\"\n",
    "\n",
    "# Neo4j schema names\n",
    "CONSTRAINT_NAME = \"removal_event_dbx_removal_id_unique\"\n",
    "VECTOR_INDEX_NAME = \"removal_reason_embeddings_dbx\"\n",
    "\n",
    "# Databricks secret scope containing Neo4j credentials\n",
    "SCOPE_NAME = \"rk-airline-neo4j-secrets\"\n",
    "\n",
    "# Source table in Unity Catalog\n",
    "SOURCE_TABLE = \"airline_test.airline_test_lakehouse.nodes_removals_large\"\n",
    "\n",
    "# Column mappings\n",
    "TEXT_COLUMN = \"RMV_REA_TX\"\n",
    "ID_COLUMN = \":ID(RemovalEvent)\"\n",
    "\n",
    "# Checkpoint location (for streaming mode when MAX_ROWS=-1)\n",
    "CHECKPOINT_LOCATION = \"/tmp/removal_embeddings_hosted_checkpoint\"\n",
    "\n",
    "# Whether to setup Neo4j schema (constraint and vector index)\n",
    "SETUP_SCHEMA = True\n",
    "\n",
    "# Whether to run similarity search test after loading\n",
    "TEST_SEARCH = True\n",
    "\n",
    "# =============================================================================\n",
    "# Step 1: Print header\n",
    "# =============================================================================\n",
    "print_section_header(\"EMBEDDING PIPELINE (DATABRICKS HOSTED MODEL)\")\n",
    "print(f\"Embedding Model: {EMBEDDING_ENDPOINT}\")\n",
    "print(f\"Dimensions: {EMBEDDING_DIMENSIONS}\")\n",
    "print(f\"Neo4j Label: :{NODE_LABEL}\")\n",
    "print(f\"Vector Index: {VECTOR_INDEX_NAME}\")\n",
    "print(f\"Max rows: {MAX_ROWS} {'(all rows)' if MAX_ROWS == -1 else ''}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 2: Load configuration\n",
    "# =============================================================================\n",
    "config = load_config(\n",
    "    dbutils,\n",
    "    SCOPE_NAME,\n",
    "    default_database=\"neo4j\",\n",
    "    default_protocol=\"neo4j+s\",\n",
    "    default_embedding_endpoint=EMBEDDING_ENDPOINT,\n",
    ")\n",
    "config.embedding_endpoint = EMBEDDING_ENDPOINT\n",
    "print_config(config, SCOPE_NAME, EMBEDDING_DIMENSIONS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpers-header-06",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "helpers-07",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def select_columns(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Select and rename columns from the source table.\"\"\"\n",
    "    return df.select(\n",
    "        col(f\"`{ID_COLUMN}`\").alias(\"removal_id\"),\n",
    "        col(TEXT_COLUMN).alias(\"removal_reason\"),\n",
    "        col(\"RMV_TRK_NO\").alias(\"rmv_trk_no\"),\n",
    "        col(\"component_id\"),\n",
    "        col(\"aircraft_id\"),\n",
    "        col(\"removal_date\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def verify_embeddings(config: Config) -> bool:\n",
    "    \"\"\"Verify embeddings were stored correctly using Spark Connector.\"\"\"\n",
    "    print_section_header(\"VERIFYING EMBEDDINGS\")\n",
    "\n",
    "    df = (\n",
    "        spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "        .option(\"url\", config.uri)\n",
    "        .option(\"authentication.basic.username\", config.username)\n",
    "        .option(\"authentication.basic.password\", config.password)\n",
    "        .option(\"database\", config.database)\n",
    "        .option(\"labels\", NODE_LABEL)\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    total_count = df.count()\n",
    "    print(f\"Total {NODE_LABEL} nodes: {total_count:,}\")\n",
    "\n",
    "    if \"embedding\" not in df.columns:\n",
    "        print(\"Warning: 'embedding' column not found!\")\n",
    "        return False\n",
    "\n",
    "    with_embeddings = df.filter(col(\"embedding\").isNotNull())\n",
    "    embedding_count = with_embeddings.count()\n",
    "    print(f\"Nodes with embeddings: {embedding_count:,}\")\n",
    "\n",
    "    print(\"\\nSample nodes:\")\n",
    "    sample = with_embeddings.limit(3).collect()\n",
    "\n",
    "    all_valid = True\n",
    "    for i, row in enumerate(sample):\n",
    "        emb = row[\"embedding\"]\n",
    "        if emb is None:\n",
    "            print(f\"  [{i+1}] No embedding\")\n",
    "            all_valid = False\n",
    "        elif len(emb) != EMBEDDING_DIMENSIONS:\n",
    "            print(f\"  [{i+1}] Wrong dimensions ({len(emb)})\")\n",
    "            all_valid = False\n",
    "        else:\n",
    "            preview = [f\"{v:.4f}\" for v in emb[:3]]\n",
    "            print(f\"  [{i+1}] {len(emb)} dims, [{', '.join(preview)}, ...]\")\n",
    "\n",
    "    return all_valid and embedding_count == total_count\n",
    "\n",
    "\n",
    "def test_vector_search(config: Config, test_text: str = \"hydraulic pump failure\"):\n",
    "    \"\"\"Test vector similarity search with the loaded embeddings.\"\"\"\n",
    "    print_section_header(\"TESTING VECTOR SEARCH\")\n",
    "\n",
    "    print(f\"Query: '{test_text}'\")\n",
    "\n",
    "    print(\"\\nGenerating query embedding...\")\n",
    "    test_embedding = generate_query_embedding(\n",
    "        EMBEDDING_ENDPOINT,\n",
    "        test_text,\n",
    "        api_format=\"hosted\",\n",
    "    )\n",
    "    print(f\"  Dimensions: {len(test_embedding)}\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "        CALL db.index.vector.queryNodes(\n",
    "            '{VECTOR_INDEX_NAME}',\n",
    "            5,\n",
    "            $embedding\n",
    "        ) YIELD node, score\n",
    "        RETURN node.removal_id AS removal_id,\n",
    "               node.removal_reason AS reason,\n",
    "               score\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nTop 5 similar items:\")\n",
    "    with neo4j_driver(config) as driver:\n",
    "        with driver.session(database=config.database) as session:\n",
    "            result = session.run(query, embedding=test_embedding)\n",
    "            records = list(result)\n",
    "\n",
    "            if not records:\n",
    "                print(\"  No results found. Is the vector index populated?\")\n",
    "            else:\n",
    "                for i, record in enumerate(records):\n",
    "                    reason = record[\"reason\"] or \"\"\n",
    "                    print(f\"  [{i+1}] Score: {record['score']:.4f}\")\n",
    "                    print(f\"      ID: {record['removal_id']}\")\n",
    "                    print(f\"      Reason: {reason}\")\n",
    "                    print()\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-header-08",
   "metadata": {},
   "source": [
    "## Run the Pipeline\n",
    "\n",
    "Execute each step below to run the embedding pipeline with Databricks hosted models."
   ]
  },
  {
   "cell_type": "code",
   "id": "run-pipeline-09",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 3: Test Neo4j connection\n",
    "if not test_neo4j_connection(config):\n",
    "    raise Exception(\"Neo4j connection failed! Check credentials and network.\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "n1fpn71sojq",
   "source": [
    "# Step 4: Validate embedding endpoint\n",
    "embedding_config = EmbeddingConfig(\n",
    "    endpoint_name=EMBEDDING_ENDPOINT,\n",
    "    dimensions=EMBEDDING_DIMENSIONS,\n",
    "    text_column=\"removal_reason\",\n",
    "    output_column=\"embedding\",\n",
    ")\n",
    "embedding_provider = DatabricksHostedEmbeddingProvider(embedding_config)\n",
    "\n",
    "print_section_header(\"VALIDATING EMBEDDING ENDPOINT\")\n",
    "if not embedding_provider.validate_endpoint():\n",
    "    raise Exception(\"Embedding endpoint validation failed!\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "zumbg8dgvkl",
   "source": [
    "# Step 5: Setup Neo4j schema (if requested)\n",
    "if SETUP_SCHEMA:\n",
    "    schema_config = SchemaConfig(\n",
    "        node_label=NODE_LABEL,\n",
    "        id_property=\"removal_id\",\n",
    "        embedding_dimensions=EMBEDDING_DIMENSIONS,\n",
    "        constraint_name=CONSTRAINT_NAME,\n",
    "        vector_index_name=VECTOR_INDEX_NAME,\n",
    "    )\n",
    "    setup_neo4j_schema(config, schema_config)\n",
    "    wait_for_vector_index(config, VECTOR_INDEX_NAME)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "srxtf0mf2dg",
   "source": [
    "# Step 6: Configure pipeline\n",
    "pipeline_config = PipelineConfig(\n",
    "    source_table=SOURCE_TABLE,\n",
    "    node_label=NODE_LABEL,\n",
    "    id_column=\"removal_id\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    write_partitions=WRITE_PARTITIONS,\n",
    "    checkpoint_location=CHECKPOINT_LOCATION,\n",
    "    max_files_per_trigger=1,\n",
    "    max_rows=MAX_ROWS,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ytz1i8hqp",
   "source": [
    "# Step 7: Run pipeline\n",
    "pipeline_start = time.time()\n",
    "\n",
    "stats = run_pipeline(\n",
    "    spark=spark,\n",
    "    neo4j_config=config,\n",
    "    pipeline_config=pipeline_config,\n",
    "    embedding_provider=embedding_provider,\n",
    "    column_selector=select_columns,\n",
    "    clear_checkpoint=True,\n",
    "    dbutils=dbutils,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ukidikp4exe",
   "source": [
    "# Step 8: Verify embeddings\n",
    "verification_passed = verify_embeddings(config)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "1oiqg6rvvql",
   "source": [
    "# Step 9: Test similarity search (if requested)\n",
    "if TEST_SEARCH:\n",
    "    test_vector_search(config)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "r7twiwxrmha",
   "source": [
    "# Step 10: Print summary\n",
    "print_pipeline_summary(stats, pipeline_config, embedding_provider)\n",
    "\n",
    "total_time = time.time() - pipeline_start\n",
    "print(f\"\\nVerification: {'PASSED' if verification_passed else 'FAILED'}\")\n",
    "print(f\"Total time: {format_duration(total_time)}\")\n",
    "print(\"\\nDone!\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "results-header-10",
   "metadata": {},
   "source": [
    "## Interpret Results\n",
    "\n",
    "### Performance Expectations\n",
    "\n",
    "| Model | Typical Throughput |\n",
    "|-------|-------------------|\n",
    "| databricks-bge-large-en | 100-300 rows/second |\n",
    "| databricks-gte-large-en | 80-200 rows/second |\n",
    "\n",
    "The embedding model is typically the bottleneck, not Neo4j writes.\n",
    "\n",
    "### Tuning Recommendations\n",
    "\n",
    "| Symptom | Adjustment |\n",
    "|---------|------------|\n",
    "| Slow embeddings | Try smaller BATCH_SIZE for better parallelism |\n",
    "| Rate limiting | Reduce BATCH_SIZE and add delays |\n",
    "| Lock contention | Reduce WRITE_PARTITIONS to 1 |\n",
    "| Long documents | Use databricks-gte-large-en for 8k context |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}