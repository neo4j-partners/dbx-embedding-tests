{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-01",
   "metadata": {},
   "source": [
    "# Neo4j Write Performance Test\n",
    "\n",
    "This notebook tests Neo4j write performance using **random embeddings** to isolate\n",
    "database throughput from embedding generation overhead.\n",
    "\n",
    "## What This Test Does\n",
    "\n",
    "1. Reads data from a Delta table\n",
    "2. Generates random 384-dimension float arrays as \"embeddings\"\n",
    "3. Writes nodes with embeddings to Neo4j\n",
    "4. Reports throughput statistics\n",
    "\n",
    "## Why Use Random Embeddings?\n",
    "\n",
    "- Establishes baseline Neo4j write throughput\n",
    "- Helps tune batch size and parallelism settings\n",
    "- Identifies if Neo4j or embedding generation is the bottleneck\n",
    "\n",
    "**Typical Results:**\n",
    "- Random embeddings: 2,000-5,000 rows/second\n",
    "- With ai_query: 100-500 rows/second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header-04",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "path-setup-00",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PATH SETUP - Ensures modules are importable from the notebook\n",
    "# =============================================================================\n",
    "# This cell adds the notebook's directory to sys.path if needed\n",
    "# Required when running in Databricks Repos or when modules aren't on the path\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the directory containing this notebook\n",
    "# In Databricks, use the notebook path to find the module directory\n",
    "try:\n",
    "    # Try to get the notebook path from Databricks context\n",
    "    notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "    notebook_dir = \"/Workspace\" + \"/\".join(notebook_path.split(\"/\")[:-1])\n",
    "    \n",
    "    if notebook_dir not in sys.path:\n",
    "        sys.path.insert(0, notebook_dir)\n",
    "        print(f\"Added to sys.path: {notebook_dir}\")\n",
    "except Exception:\n",
    "    # Fallback: add current working directory\n",
    "    cwd = os.getcwd()\n",
    "    if cwd not in sys.path:\n",
    "        sys.path.insert(0, cwd)\n",
    "        print(f\"Added to sys.path: {cwd}\")\n",
    "\n",
    "print(\"Path setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "imports-05",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Import from modular components\n",
    "from load_utils import (\n",
    "    Config,\n",
    "    load_config,\n",
    "    neo4j_driver,\n",
    "    print_config,\n",
    "    print_section_header,\n",
    "    test_neo4j_connection,\n",
    "    format_duration,\n",
    ")\n",
    "\n",
    "from neo4j_schema import (\n",
    "    SchemaConfig,\n",
    "    setup_neo4j_schema,\n",
    "    delete_nodes_by_label,\n",
    ")\n",
    "\n",
    "from embedding_providers import (\n",
    "    EmbeddingConfig,\n",
    "    RandomEmbeddingProvider,\n",
    ")\n",
    "\n",
    "from streaming_pipeline import (\n",
    "    PipelineConfig,\n",
    "    run_pipeline,\n",
    "    print_pipeline_summary,\n",
    ")\n",
    "\n",
    "print(\"Imports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header-02",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Adjust these settings to control the test run."
   ]
  },
  {
   "cell_type": "code",
   "id": "config-03",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEST CONFIGURATION - Modify these values to control the test\n",
    "# =============================================================================\n",
    "\n",
    "# Row limit: Set to a positive number to limit rows, or -1 for all rows\n",
    "# Default: 500 rows for quick testing\n",
    "MAX_ROWS = 500  # Set to -1 to process all rows\n",
    "\n",
    "# Batch size: Number of rows per Neo4j transaction\n",
    "# Larger = faster but more memory; Smaller = safer but slower\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "# Write partitions: Parallel writers to Neo4j\n",
    "# 1 = serial (safest), 2-4 = moderate parallelism, 4+ = may cause lock contention\n",
    "WRITE_PARTITIONS = 1\n",
    "\n",
    "# Embedding dimensions (must match production settings)\n",
    "EMBEDDING_DIMENSIONS = 384\n",
    "\n",
    "# Neo4j node label for test data (separate from production)\n",
    "TEST_LABEL = \"RemovalEventTest\"\n",
    "\n",
    "# Databricks secret scope containing Neo4j credentials\n",
    "SCOPE_NAME = \"airline-neo4j-secrets\"\n",
    "\n",
    "# Source table in Unity Catalog\n",
    "SOURCE_TABLE = \"airline_test.airline_test_lakehouse.nodes_removals_large\"\n",
    "\n",
    "# Column mappings\n",
    "TEXT_COLUMN = \"RMV_REA_TX\"\n",
    "ID_COLUMN = \":ID(RemovalEvent)\"\n",
    "\n",
    "# Checkpoint location (for streaming mode when MAX_ROWS=-1)\n",
    "CHECKPOINT_LOCATION = \"/tmp/neo4j_write_test_checkpoint\"\n",
    "\n",
    "# Whether to clean up existing test nodes before running\n",
    "CLEANUP_NODES = True\n",
    "\n",
    "# Whether to setup Neo4j schema (constraint)\n",
    "SETUP_SCHEMA = True\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"  MAX_ROWS: {MAX_ROWS} {'(all rows)' if MAX_ROWS == -1 else ''}\")\n",
    "print(f\"  BATCH_SIZE: {BATCH_SIZE:,}\")\n",
    "print(f\"  WRITE_PARTITIONS: {WRITE_PARTITIONS}\")\n",
    "print(f\"  TEST_LABEL: {TEST_LABEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpers-header-06",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "helpers-07",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def select_columns(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Select and rename columns from the source table.\"\"\"\n",
    "    return df.select(\n",
    "        col(f\"`{ID_COLUMN}`\").alias(\"removal_id\"),\n",
    "        col(TEXT_COLUMN).alias(\"removal_reason\"),\n",
    "        col(\"RMV_TRK_NO\").alias(\"rmv_trk_no\"),\n",
    "        col(\"component_id\"),\n",
    "        col(\"aircraft_id\"),\n",
    "        col(\"removal_date\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def verify_test_nodes(config: Config) -> int:\n",
    "    \"\"\"Verify test nodes were created correctly in Neo4j.\"\"\"\n",
    "    print_section_header(\"VERIFYING TEST NODES\")\n",
    "\n",
    "    with neo4j_driver(config) as driver:\n",
    "        with driver.session(database=config.database) as session:\n",
    "            # Count total nodes\n",
    "            result = session.run(f\"\"\"\n",
    "                MATCH (n:{TEST_LABEL})\n",
    "                RETURN count(n) AS count\n",
    "            \"\"\")\n",
    "            total_count = result.single()[\"count\"]\n",
    "            print(f\"  Total {TEST_LABEL} nodes: {total_count:,}\")\n",
    "\n",
    "            # Count nodes with embeddings\n",
    "            result = session.run(f\"\"\"\n",
    "                MATCH (n:{TEST_LABEL})\n",
    "                WHERE n.embedding IS NOT NULL\n",
    "                RETURN count(n) AS count\n",
    "            \"\"\")\n",
    "            embedding_count = result.single()[\"count\"]\n",
    "            print(f\"  Nodes with embeddings: {embedding_count:,}\")\n",
    "\n",
    "            # Sample verification\n",
    "            result = session.run(f\"\"\"\n",
    "                MATCH (n:{TEST_LABEL})\n",
    "                WHERE n.embedding IS NOT NULL\n",
    "                RETURN n.removal_id AS id, size(n.embedding) AS dims\n",
    "                LIMIT 3\n",
    "            \"\"\")\n",
    "            records = list(result)\n",
    "            if records:\n",
    "                print(\"\\n  Sample nodes:\")\n",
    "                for r in records:\n",
    "                    print(f\"    ID: {r['id']}, embedding dims: {r['dims']}\")\n",
    "\n",
    "    return total_count\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-header-08",
   "metadata": {},
   "source": [
    "## Run the Test\n",
    "\n",
    "Execute the cell below to run the Neo4j write performance test."
   ]
  },
  {
   "cell_type": "code",
   "id": "run-test-09",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN NEO4J WRITE PERFORMANCE TEST\n",
    "# =============================================================================\n",
    "\n",
    "pipeline_start = time.time()\n",
    "\n",
    "# Step 1: Print header\n",
    "print_section_header(\"NEO4J WRITE PERFORMANCE TEST\")\n",
    "print(\"Testing Neo4j write performance with random embeddings\")\n",
    "print(f\"Target label: :{TEST_LABEL}\")\n",
    "print(f\"Embedding dimensions: {EMBEDDING_DIMENSIONS} (random)\")\n",
    "print(f\"Max rows: {MAX_ROWS} {'(all rows)' if MAX_ROWS == -1 else ''}\")\n",
    "\n",
    "# Step 2: Load configuration\n",
    "config = load_config(\n",
    "    dbutils,\n",
    "    SCOPE_NAME,\n",
    "    default_database=\"neo4j\",\n",
    "    default_protocol=\"neo4j+s\",\n",
    "    default_embedding_endpoint=\"unused\",\n",
    ")\n",
    "print_config(config, SCOPE_NAME, EMBEDDING_DIMENSIONS, BATCH_SIZE)\n",
    "\n",
    "# Step 3: Test Neo4j connection\n",
    "if not test_neo4j_connection(config):\n",
    "    raise Exception(\"Neo4j connection failed! Check credentials and network.\")\n",
    "\n",
    "# Step 4: Setup schema (if requested)\n",
    "if SETUP_SCHEMA:\n",
    "    schema_config = SchemaConfig(\n",
    "        node_label=TEST_LABEL,\n",
    "        id_property=\"removal_id\",\n",
    "        embedding_dimensions=EMBEDDING_DIMENSIONS,\n",
    "        constraint_name=f\"{TEST_LABEL.lower()}_removal_id_unique\",\n",
    "        vector_index_name=f\"{TEST_LABEL.lower()}_embeddings\",\n",
    "    )\n",
    "    setup_neo4j_schema(config, schema_config)\n",
    "\n",
    "# Step 5: Cleanup existing test nodes (if requested)\n",
    "if CLEANUP_NODES:\n",
    "    delete_nodes_by_label(config, TEST_LABEL)\n",
    "\n",
    "# Step 6: Configure embedding provider (random)\n",
    "embedding_config = EmbeddingConfig(\n",
    "    endpoint_name=\"random\",\n",
    "    dimensions=EMBEDDING_DIMENSIONS,\n",
    "    text_column=\"removal_reason\",\n",
    "    output_column=\"embedding\",\n",
    ")\n",
    "embedding_provider = RandomEmbeddingProvider(embedding_config)\n",
    "\n",
    "print_section_header(\"VALIDATING EMBEDDING PROVIDER\")\n",
    "embedding_provider.validate_endpoint()\n",
    "\n",
    "# Step 7: Configure pipeline\n",
    "pipeline_config = PipelineConfig(\n",
    "    source_table=SOURCE_TABLE,\n",
    "    node_label=TEST_LABEL,\n",
    "    id_column=\"removal_id\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    write_partitions=WRITE_PARTITIONS,\n",
    "    checkpoint_location=CHECKPOINT_LOCATION,\n",
    "    max_files_per_trigger=1,\n",
    "    max_rows=MAX_ROWS,\n",
    ")\n",
    "\n",
    "# Step 8: Run pipeline\n",
    "stats = run_pipeline(\n",
    "    spark=spark,\n",
    "    neo4j_config=config,\n",
    "    pipeline_config=pipeline_config,\n",
    "    embedding_provider=embedding_provider,\n",
    "    column_selector=select_columns,\n",
    "    clear_checkpoint=True,\n",
    "    dbutils=dbutils,\n",
    ")\n",
    "\n",
    "# Step 9: Verify results\n",
    "verify_test_nodes(config)\n",
    "\n",
    "# Step 10: Print summary\n",
    "print_pipeline_summary(stats, pipeline_config, embedding_provider)\n",
    "\n",
    "total_time = time.time() - pipeline_start\n",
    "print(f\"\\nTotal test time: {format_duration(total_time)}\")\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header-10",
   "metadata": {},
   "source": [
    "## Interpret Results\n",
    "\n",
    "After running the test, review the statistics:\n",
    "\n",
    "- **Rows per second**: This is your Neo4j write throughput baseline\n",
    "- **Batch time**: Time to process each chunk (should be consistent)\n",
    "- **Total nodes**: Verify all expected nodes were created\n",
    "\n",
    "### Tuning Recommendations\n",
    "\n",
    "| Symptom | Adjustment |\n",
    "|---------|------------|\n",
    "| Low throughput | Increase BATCH_SIZE (try 10000) |\n",
    "| Memory errors | Decrease BATCH_SIZE (try 1000) |\n",
    "| Lock contention | Decrease WRITE_PARTITIONS to 1 |\n",
    "| Need more speed | Increase WRITE_PARTITIONS (try 2-4) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
