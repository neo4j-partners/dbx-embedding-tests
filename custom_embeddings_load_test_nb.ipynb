{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-01",
   "metadata": {},
   "source": [
    "# Embedding Pipeline - Custom Model Endpoints\n",
    "\n",
    "This notebook loads data from a Delta table into Neo4j with vector embeddings\n",
    "using a **custom model** deployed to Databricks Model Serving.\n",
    "\n",
    "## Custom Model Advantages\n",
    "\n",
    "- **Control**: Choose any sentence-transformer model\n",
    "- **Fine-tuning**: Use domain-specific models\n",
    "- **Dimensions**: Select appropriate embedding size\n",
    "- **Cost**: May be cheaper for high volume\n",
    "\n",
    "## Common Model Dimensions\n",
    "\n",
    "| Model | Dimensions | Notes |\n",
    "|-------|------------|-------|\n",
    "| all-MiniLM-L6-v2 | 384 | Fast, good quality |\n",
    "| all-mpnet-base-v2 | 768 | Higher quality |\n",
    "| e5-large-v2 | 1024 | Highest quality |\n",
    "\n",
    "## API Format\n",
    "\n",
    "Custom models use the `dataframe_records` input format:\n",
    "\n",
    "```python\n",
    "Input:  {\"dataframe_records\": [{\"text\": \"hello\"}]}\n",
    "Output: {\"predictions\": [[0.1, 0.2, ...]]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header-04",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "path-setup-00",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PATH SETUP - Ensures modules are importable from the notebook\n",
    "# =============================================================================\n",
    "# This cell adds the notebook's directory to sys.path if needed\n",
    "# Required when running in Databricks Repos or when modules aren't on the path\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the directory containing this notebook\n",
    "# In Databricks, use the notebook path to find the module directory\n",
    "try:\n",
    "    # Try to get the notebook path from Databricks context\n",
    "    notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "    notebook_dir = \"/Workspace\" + \"/\".join(notebook_path.split(\"/\")[:-1])\n",
    "    \n",
    "    if notebook_dir not in sys.path:\n",
    "        sys.path.insert(0, notebook_dir)\n",
    "        print(f\"Added to sys.path: {notebook_dir}\")\n",
    "except Exception:\n",
    "    # Fallback: add current working directory\n",
    "    cwd = os.getcwd()\n",
    "    if cwd not in sys.path:\n",
    "        sys.path.insert(0, cwd)\n",
    "        print(f\"Added to sys.path: {cwd}\")\n",
    "\n",
    "print(\"Path setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "imports-05",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Import from modular components\n",
    "from load_utils import (\n",
    "    Config,\n",
    "    load_config,\n",
    "    neo4j_driver,\n",
    "    print_config,\n",
    "    print_section_header,\n",
    "    test_neo4j_connection,\n",
    "    format_duration,\n",
    ")\n",
    "\n",
    "from neo4j_schema import (\n",
    "    SchemaConfig,\n",
    "    setup_neo4j_schema,\n",
    "    wait_for_vector_index,\n",
    ")\n",
    "\n",
    "from embedding_providers import (\n",
    "    EmbeddingConfig,\n",
    "    CustomModelEmbeddingProvider,\n",
    "    generate_query_embedding,\n",
    ")\n",
    "\n",
    "from streaming_pipeline import (\n",
    "    PipelineConfig,\n",
    "    run_pipeline,\n",
    "    print_pipeline_summary,\n",
    ")\n",
    "\n",
    "print(\"Imports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header-02",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Adjust these settings to control the embedding pipeline."
   ]
  },
  {
   "cell_type": "code",
   "id": "config-03",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE CONFIGURATION - Modify these values to control the run\n",
    "# =============================================================================\n",
    "\n",
    "# Row limit: Set to a positive number to limit rows, or -1 for all rows\n",
    "# Default: 500 rows for quick testing\n",
    "MAX_ROWS = 500  # Set to -1 to process all rows\n",
    "\n",
    "# Batch size: Number of rows per Neo4j transaction\n",
    "# Larger = faster but more memory; Smaller = safer but slower\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "# Write partitions: Parallel writers to Neo4j\n",
    "# Custom models often handle concurrency well, so 4 is a good starting point\n",
    "WRITE_PARTITIONS = 4\n",
    "\n",
    "# Custom embedding model endpoint (your deployed model)\n",
    "EMBEDDING_ENDPOINT = \"rk_serving_airline_embedding\"\n",
    "\n",
    "# Embedding dimensions (must match your model!)\n",
    "# MiniLM: 384, MPNet: 768, E5-Large: 1024\n",
    "EMBEDDING_DIMENSIONS = 384\n",
    "\n",
    "# Neo4j node label (separate from DBX hosted model to avoid conflicts)\n",
    "NODE_LABEL = \"RemovalEvent\"\n",
    "\n",
    "# Neo4j schema names\n",
    "CONSTRAINT_NAME = \"removal_event_removal_id_unique\"\n",
    "VECTOR_INDEX_NAME = \"removal_reason_embeddings\"\n",
    "\n",
    "# Databricks secret scope containing Neo4j credentials\n",
    "SCOPE_NAME = \"airline-neo4j-secrets\"\n",
    "\n",
    "# Source table in Unity Catalog\n",
    "SOURCE_TABLE = \"airline_test.airline_test_lakehouse.nodes_removals_large\"\n",
    "\n",
    "# Column mappings\n",
    "TEXT_COLUMN = \"RMV_REA_TX\"\n",
    "ID_COLUMN = \":ID(RemovalEvent)\"\n",
    "\n",
    "# Checkpoint location (for streaming mode when MAX_ROWS=-1)\n",
    "CHECKPOINT_LOCATION = \"/tmp/removal_embeddings_checkpoint\"\n",
    "\n",
    "# Whether to setup Neo4j schema (constraint and vector index)\n",
    "SETUP_SCHEMA = True\n",
    "\n",
    "# Whether to run similarity search test after loading\n",
    "TEST_SEARCH = True\n",
    "\n",
    "# =============================================================================\n",
    "# Step 1: Print header\n",
    "# =============================================================================\n",
    "print_section_header(\"EMBEDDING PIPELINE (CUSTOM MODEL)\")\n",
    "print(f\"Embedding Model: {EMBEDDING_ENDPOINT}\")\n",
    "print(f\"Dimensions: {EMBEDDING_DIMENSIONS}\")\n",
    "print(f\"Neo4j Label: :{NODE_LABEL}\")\n",
    "print(f\"Vector Index: {VECTOR_INDEX_NAME}\")\n",
    "print(f\"Max rows: {MAX_ROWS} {'(all rows)' if MAX_ROWS == -1 else ''}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 2: Load configuration\n",
    "# =============================================================================\n",
    "config = load_config(\n",
    "    dbutils,\n",
    "    SCOPE_NAME,\n",
    "    default_database=\"neo4j\",\n",
    "    default_protocol=\"neo4j+s\",\n",
    "    default_embedding_endpoint=EMBEDDING_ENDPOINT,\n",
    ")\n",
    "config.embedding_endpoint = EMBEDDING_ENDPOINT\n",
    "print_config(config, SCOPE_NAME, EMBEDDING_DIMENSIONS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpers-header-06",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "helpers-07",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def select_columns(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Select and rename columns from the source table.\"\"\"\n",
    "    return df.select(\n",
    "        col(f\"`{ID_COLUMN}`\").alias(\"removal_id\"),\n",
    "        col(TEXT_COLUMN).alias(\"removal_reason\"),\n",
    "        col(\"RMV_TRK_NO\").alias(\"rmv_trk_no\"),\n",
    "        col(\"component_id\"),\n",
    "        col(\"aircraft_id\"),\n",
    "        col(\"removal_date\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def verify_embeddings(config: Config) -> bool:\n",
    "    \"\"\"Verify embeddings were stored correctly using Spark Connector.\"\"\"\n",
    "    print_section_header(\"VERIFYING EMBEDDINGS\")\n",
    "\n",
    "    df = (\n",
    "        spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "        .option(\"url\", config.uri)\n",
    "        .option(\"authentication.basic.username\", config.username)\n",
    "        .option(\"authentication.basic.password\", config.password)\n",
    "        .option(\"database\", config.database)\n",
    "        .option(\"labels\", NODE_LABEL)\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    total_count = df.count()\n",
    "    print(f\"Total {NODE_LABEL} nodes: {total_count:,}\")\n",
    "\n",
    "    if \"embedding\" not in df.columns:\n",
    "        print(\"Warning: 'embedding' column not found!\")\n",
    "        return False\n",
    "\n",
    "    with_embeddings = df.filter(col(\"embedding\").isNotNull())\n",
    "    embedding_count = with_embeddings.count()\n",
    "    print(f\"Nodes with embeddings: {embedding_count:,}\")\n",
    "\n",
    "    print(\"\\nSample nodes:\")\n",
    "    sample = with_embeddings.limit(3).collect()\n",
    "\n",
    "    all_valid = True\n",
    "    for i, row in enumerate(sample):\n",
    "        emb = row[\"embedding\"]\n",
    "        if emb is None:\n",
    "            print(f\"  [{i+1}] No embedding\")\n",
    "            all_valid = False\n",
    "        elif len(emb) != EMBEDDING_DIMENSIONS:\n",
    "            print(f\"  [{i+1}] Wrong dimensions ({len(emb)})\")\n",
    "            all_valid = False\n",
    "        else:\n",
    "            preview = [f\"{v:.4f}\" for v in emb[:3]]\n",
    "            print(f\"  [{i+1}] {len(emb)} dims, [{', '.join(preview)}, ...]\")\n",
    "\n",
    "    return all_valid and embedding_count == total_count\n",
    "\n",
    "\n",
    "def test_vector_search(config: Config, test_text: str = \"hydraulic pump failure\"):\n",
    "    \"\"\"Test vector similarity search with the loaded embeddings.\"\"\"\n",
    "    print_section_header(\"TESTING VECTOR SEARCH\")\n",
    "\n",
    "    print(f\"Query: '{test_text}'\")\n",
    "\n",
    "    print(\"\\nGenerating query embedding...\")\n",
    "    test_embedding = generate_query_embedding(\n",
    "        EMBEDDING_ENDPOINT,\n",
    "        test_text,\n",
    "        api_format=\"custom\",  # Custom model format\n",
    "    )\n",
    "    print(f\"  Dimensions: {len(test_embedding)}\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "        CALL db.index.vector.queryNodes(\n",
    "            '{VECTOR_INDEX_NAME}',\n",
    "            5,\n",
    "            $embedding\n",
    "        ) YIELD node, score\n",
    "        RETURN node.removal_id AS removal_id,\n",
    "               node.removal_reason AS reason,\n",
    "               score\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nTop 5 similar items:\")\n",
    "    with neo4j_driver(config) as driver:\n",
    "        with driver.session(database=config.database) as session:\n",
    "            result = session.run(query, embedding=test_embedding)\n",
    "            records = list(result)\n",
    "\n",
    "            if not records:\n",
    "                print(\"  No results found. Is the vector index populated?\")\n",
    "            else:\n",
    "                for i, record in enumerate(records):\n",
    "                    reason = record[\"reason\"] or \"\"\n",
    "                    print(f\"  [{i+1}] Score: {record['score']:.4f}\")\n",
    "                    print(f\"      ID: {record['removal_id']}\")\n",
    "                    print(f\"      Reason: {reason}\")\n",
    "                    print()\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-header-08",
   "metadata": {},
   "source": [
    "## Run the Pipeline\n",
    "\n",
    "Execute each step below to run the embedding pipeline with your custom model."
   ]
  },
  {
   "cell_type": "code",
   "id": "run-pipeline-09",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 3: Test Neo4j connection\n",
    "if not test_neo4j_connection(config):\n",
    "    raise Exception(\"Neo4j connection failed! Check credentials and network.\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "1ccsdeeyya",
   "source": [
    "# Step 4: Validate embedding endpoint\n",
    "embedding_config = EmbeddingConfig(\n",
    "    endpoint_name=EMBEDDING_ENDPOINT,\n",
    "    dimensions=EMBEDDING_DIMENSIONS,\n",
    "    text_column=\"removal_reason\",\n",
    "    output_column=\"embedding\",\n",
    ")\n",
    "embedding_provider = CustomModelEmbeddingProvider(embedding_config)\n",
    "\n",
    "print_section_header(\"VALIDATING EMBEDDING ENDPOINT\")\n",
    "if not embedding_provider.validate_endpoint():\n",
    "    raise Exception(\n",
    "        f\"Embedding endpoint validation failed!\\n\"\n",
    "        f\"Check that your Model Serving endpoint '{EMBEDDING_ENDPOINT}' is running\\n\"\n",
    "        f\"and EMBEDDING_DIMENSIONS ({EMBEDDING_DIMENSIONS}) matches your model.\"\n",
    "    )"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ga3uifdfq0j",
   "source": [
    "# Step 5: Setup Neo4j schema (if requested)\n",
    "if SETUP_SCHEMA:\n",
    "    schema_config = SchemaConfig(\n",
    "        node_label=NODE_LABEL,\n",
    "        id_property=\"removal_id\",\n",
    "        embedding_dimensions=EMBEDDING_DIMENSIONS,\n",
    "        constraint_name=CONSTRAINT_NAME,\n",
    "        vector_index_name=VECTOR_INDEX_NAME,\n",
    "    )\n",
    "    setup_neo4j_schema(config, schema_config)\n",
    "    wait_for_vector_index(config, VECTOR_INDEX_NAME)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "r9fddfv358m",
   "source": [
    "# Step 6: Configure pipeline\n",
    "pipeline_config = PipelineConfig(\n",
    "    source_table=SOURCE_TABLE,\n",
    "    node_label=NODE_LABEL,\n",
    "    id_column=\"removal_id\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    write_partitions=WRITE_PARTITIONS,\n",
    "    checkpoint_location=CHECKPOINT_LOCATION,\n",
    "    max_files_per_trigger=1,\n",
    "    max_rows=MAX_ROWS,\n",
    ")\n",
    "\n",
    "# Validate max_rows was set correctly\n",
    "print(f\"PipelineConfig created: max_rows={pipeline_config.max_rows}\")\n",
    "if MAX_ROWS > 0 and pipeline_config.max_rows != MAX_ROWS:\n",
    "    raise ValueError(f\"max_rows mismatch: expected {MAX_ROWS}, got {pipeline_config.max_rows}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "i50notz0qcq",
   "source": [
    "# Step 7: Run pipeline\n",
    "pipeline_start = time.time()\n",
    "\n",
    "stats = run_pipeline(\n",
    "    spark=spark,\n",
    "    neo4j_config=config,\n",
    "    pipeline_config=pipeline_config,\n",
    "    embedding_provider=embedding_provider,\n",
    "    column_selector=select_columns,\n",
    "    clear_checkpoint=True,  # Fresh start\n",
    "    dbutils=dbutils,\n",
    ")\n",
    "\n",
    "pipeline_duration = time.time() - pipeline_start\n",
    "print(f\"\\nPipeline completed in {format_duration(pipeline_duration)}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "g14vyy3ox4e",
   "source": [
    "# Step 8: Verify embeddings\n",
    "verification_passed = verify_embeddings(config)\n",
    "print(f\"\\nVerification: {'PASSED' if verification_passed else 'FAILED'}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "wshxhoa59mq",
   "source": [
    "# Step 9: Test similarity search (if requested)\n",
    "if TEST_SEARCH:\n",
    "    test_vector_search(config)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "wpsbgph6ug",
   "source": [
    "# Step 10: Print summary\n",
    "print_pipeline_summary(stats, pipeline_config, embedding_provider)\n",
    "print(\"\\nDone!\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "results-header-10",
   "metadata": {},
   "source": [
    "## Interpret Results\n",
    "\n",
    "### Performance Expectations\n",
    "\n",
    "| Model | Typical Throughput |\n",
    "|-------|-------------------|\n",
    "| MiniLM-L6 (384d) | 200-500 rows/second |\n",
    "| MPNet (768d) | 100-300 rows/second |\n",
    "| E5-Large (1024d) | 50-150 rows/second |\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "| Error | Solution |\n",
    "|-------|----------|\n",
    "| Dimension mismatch | Update EMBEDDING_DIMENSIONS to match your model |\n",
    "| Endpoint not found | Check Model Serving endpoint is running |\n",
    "| Rate limiting | Reduce BATCH_SIZE |\n",
    "| Lock contention | Reduce WRITE_PARTITIONS |\n",
    "\n",
    "### Custom Model Checklist\n",
    "\n",
    "- [ ] Model Serving endpoint is running\n",
    "- [ ] EMBEDDING_ENDPOINT matches your endpoint name\n",
    "- [ ] EMBEDDING_DIMENSIONS matches your model's output\n",
    "- [ ] Input format uses `dataframe_records` with `text` field"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
